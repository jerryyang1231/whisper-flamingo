# Video
- [The LRS3 dataset](https://mmai.io/datasets/lip_reading/) was pre-processed using the [MuAViC codebase](https://github.com/facebookresearch/muavic). The extracted lip-videos are nearly the same as using the [AV-HuBERT preparation code](https://github.com/facebookresearch/av_hubert/tree/main/avhubert/preparation#audio-noise-preparation-optional), except [slightly more video compression is applied](https://github.com/facebookresearch/muavic/issues/13), which slightly reduces the visual quality. In practice, it should be fine to use the lip-videos processed using the AV-HuBERT or MuAViC codebase.

# Noise
- We followed the [instructions in AV-HuBERT](https://github.com/facebookresearch/av_hubert/tree/main/avhubert/preparation#audio-noise-preparation-optional) to prepare MUSAN and LRS3 Noise 
    - For training, we use ${musan}/tsv/allÂ including MUSAN babble, music, noise and LRS3 speech.
- Based on [Robust AV-HuBERT](https://arxiv.org/abs/2201.01763) we use LRS3 babble noise for english AVSR evaluation, not MUSAN babble noise.
    - The AV-HuBERT script `lrs3_noise.py` generates a single babble noise file using random samples. We released our generated noise file for reproducible testing.
    - Note that MUSAN babble noise generated by `musan_prepare.py` generates many babble noise files, making results [slightly different each time the model is decoded](https://github.com/facebookresearch/av_hubert/issues/78). 
- For En-X translation, we follow MuAViC results and generate multilingual babble noise from MuAViC audio.
    - The exact script we used is provided as `make_noise_muavic.ipynb` and we also released the noise file.
    - We used 30 speakers with minimum audio length of 15s and ensured at least 3 speakers per language.
